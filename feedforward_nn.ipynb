{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sigmoid(x, inverse=False):\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    if not inverse:\n",
    "        return result\n",
    "    else:\n",
    "        return result * (1 - result)\n",
    "\n",
    "class FeedForwardNetwork:\n",
    "    def __init__(self, arch=np.array([784, 160, 60, 10]), lr=0.001, batch_size=200):\n",
    "        self._arch = arch\n",
    "        self.batch_size = batch_size\n",
    "        self._lr = lr\n",
    "        self._num_layers = len(self._arch)\n",
    "        self._init_w()\n",
    "        self._init_b()\n",
    "        self.weight_decay = 0.2\n",
    "\n",
    "    def _init_w(self):\n",
    "        ''' Initialize weights using gaussian distribution '''\n",
    "        self._w = [np.random.randn(j, i) for i, j in zip(self._arch[:-1], self._arch[1:])]\n",
    "\n",
    "    def _init_b(self):\n",
    "        ''' Initialize biases using gaussian distribution '''\n",
    "        self._b = [np.random.randn(i, 1) for i in self._arch[1:]]\n",
    "\n",
    "    def _calculate_z(self, x, n):\n",
    "        ''' Calculate raw output value z at layer n '''\n",
    "        return np.dot(self._w[n], x) + self._b[n]\n",
    "\n",
    "    def _propagate(self, x, return_label=False):\n",
    "        ''' Calculate the activation and output values at each layer given input x '''\n",
    "        self._a = []\n",
    "        self._o = []\n",
    "        self._a.append(x)\n",
    "        self._o.append(x)\n",
    "        tmp = x\n",
    "\n",
    "        num_neuron_layers = self._num_layers - 1\n",
    "        output_layer = num_neuron_layers - 1\n",
    "\n",
    "        for layer in range(num_neuron_layers):\n",
    "            if layer == output_layer:\n",
    "                activate = sigmoid\n",
    "            else:\n",
    "                activate = sigmoid\n",
    "\n",
    "            tmp = self._calculate_z(tmp, layer)\n",
    "            self._o.append(tmp)\n",
    "            tmp = activate(tmp)\n",
    "            self._a.append(tmp)\n",
    "\n",
    "        if return_label:\n",
    "            return tmp\n",
    "        else:\n",
    "            return self._a, self._o\n",
    "\n",
    "    def _backpropagate(self, y):\n",
    "        ''' Propagate error signal backward from output layer '''\n",
    "        self._d = []\n",
    "\n",
    "        ''' Pick activation functions for hidden and output layers '''\n",
    "        output_layer_activate = sigmoid\n",
    "        hidden_layer_activate = sigmoid\n",
    "\n",
    "        ''' Calculating the gradient for the output layer l = n '''\n",
    "        if output_layer_activate == sigmoid:\n",
    "            out_d = np.multiply(\n",
    "                self._a[-1] - y,\n",
    "                output_layer_activate(self._o[-1], inverse=True)\n",
    "            )\n",
    "        elif output_layer_activate == softmax:\n",
    "            out_d = self._a[-1] - y\n",
    "\n",
    "        self._d.append(out_d)\n",
    "\n",
    "        ''' Calculating the gradient for all layers l = 0 ... n-1 '''\n",
    "        num_hidden_layers = self._num_layers - 2\n",
    "        for l in range(num_hidden_layers):\n",
    "            weights_l = (self._w[-(l+1)]).T\n",
    "            delta_l = self._d[l]\n",
    "            d = np.multiply(np.dot(weights_l, delta_l), hidden_layer_activate(self._o[-(l+2)], inverse=True))\n",
    "            self._d.append(d)\n",
    "\n",
    "        ''' Reversal of the list of gradients '''\n",
    "        self._d = self._d[::-1]\n",
    "        return self._d\n",
    "\n",
    "    def _adjust_weights(self):\n",
    "        ''' Adjust weights according to gradients self._d '''\n",
    "        learn_rate = self._lr\n",
    "        new_weights = []\n",
    "\n",
    "        for weight, delta, activation in zip(self._w, self._d, self._a):\n",
    "            regularization = (learn_rate * self.weight_decay) * weight\n",
    "            new_w = weight - learn_rate * np.dot(delta, activation.T) - regularization\n",
    "            new_weights.append(new_w)\n",
    "\n",
    "        self._w = new_weights\n",
    "        return self._w\n",
    "\n",
    "    def _adjust_biases(self):\n",
    "        ''' Adjust biases according to gradients self._d '''\n",
    "        learn_rate = self._lr\n",
    "        new_biases = []\n",
    "\n",
    "        for bias, delta in zip(self._b, self._d):\n",
    "            regularization = (learn_rate * self.weight_decay) * bias\n",
    "            new_b = bias - learn_rate * (np.sum(delta, axis=1)).reshape(bias.shape) - regularization\n",
    "            new_biases.append(new_b)\n",
    "\n",
    "        self._b = new_biases\n",
    "        return self._b\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_y(y):\n",
    "        # Re-implementation of _prepare_train_targets to match our current data format\n",
    "        a = np.zeros((y.shape[0], 10, 1))\n",
    "\n",
    "        for i, label in enumerate(y):\n",
    "            a[i][label] = 1\n",
    "            a[i] = a[i].reshape(10,1)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def test_network(self, test_input, test_labels):\n",
    "        acc = []\n",
    "\n",
    "        output_labels = self._propagate(test_input, return_label=True).T\n",
    "        num_images = output_labels.shape[0]\n",
    "\n",
    "        for i in range(num_images):\n",
    "            output = np.argmax(output_labels[i])\n",
    "            if output == test_labels[i]:\n",
    "                acc.append(1)\n",
    "\n",
    "        return float(len(acc)) / float(num_images)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_train = 60000\n",
    "    n_test = 10000\n",
    "    k_folds = 12\n",
    "    fold_size = n_train/k_folds\n",
    "    epochs = 250\n",
    "    alpha = 0.005\n",
    "    batch_size = 200\n",
    "    seed = random.random_seed(0)\n",
    "    \n",
    "    net = FeedForwardNetwork(arch=[784,160,60,10], lr=alpha, batch_size=batch_size)\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    \n",
    "    indices = range(len(mnist.data))\n",
    "    \n",
    "    train_idx = range(0,n_train)\n",
    "    test_idx = range(n_train+1,n_train+n_test)\n",
    "    \n",
    "    X_train, y_train = mnist.data[train_idx], mnist.target[train_idx]\n",
    "    X_test, y_test = mnist.data[test_idx], mnist.target[test_idx]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train, random_state=seed)\n",
    "        \n",
    "        for i in range(n_train/batch_size):\n",
    "            \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
